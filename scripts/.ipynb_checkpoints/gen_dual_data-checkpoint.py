# æª”æ¡ˆä½ç½®: scripts/gen_dual_data.py
import os
import sys
import torch
import numpy as np
import json
import time
from PIL import Image
from omegaconf import OmegaConf
from torch import autocast
from tqdm import tqdm
from reedsolo import RSCodec # æ‚¨çš„ alice_gen.py ç”¨åˆ°äº†é€™å€‹

# === è·¯å¾‘èˆ‡ç’°å¢ƒè¨­å®š ===
# å–å¾— scripts è³‡æ–™å¤¾çš„è·¯å¾‘
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# å–å¾—å°ˆæ¡ˆæ ¹ç›®éŒ„ (mas_GRDH)
PROJECT_ROOT = os.path.dirname(CURRENT_DIR)

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ sys.pathï¼Œä»¥ä¾¿å°å…¥ text_stego_module å’Œ ldm
sys.path.append(PROJECT_ROOT)

try:
    from ldm.util import instantiate_from_config
    from ldm.models.diffusion.dpm_solver import DPMSolverSampler
    import mapping_module
    # å°å…¥æ‚¨çš„æ–‡å­—éš±å¯«æ¨¡çµ„
    from text_stego_module.stego import TextStegoSystem
except ImportError as e:
    print(f"âŒ å°å…¥å¤±æ•—: {e}")
    print(f"è«‹ç¢ºèªæ‚¨ä½æ–¼ mas_GRDH/scripts ç›®éŒ„ä¸‹åŸ·è¡Œï¼Œä¸” {PROJECT_ROOT} åŒ…å«å¿…è¦çš„æ¨¡çµ„ã€‚")
    sys.exit(1)

# === å…¨åŸŸè¨­å®š ===
CKPT_PATH = "/home/vcpuser/netdrive/Workspace/stt/mas_GRDH/weights/v1-5-pruned.ckpt"
CONFIG_PATH = os.path.join(PROJECT_ROOT, "configs/stable-diffusion/ldm.yaml")
GPT2_PATH = os.path.join(PROJECT_ROOT, "gpt2") # æ‚¨çš„ GPT-2 è·¯å¾‘
OUTPUT_ROOT = os.path.join(PROJECT_ROOT, "training_data")
COCO_JSON_PATH = os.path.join(PROJECT_ROOT, "annotations/captions_val2017.json")

# ç”Ÿæˆæ•¸é‡
NUM_SAMPLES = 5000 

# === ECC åƒæ•¸ (åƒè€ƒæ‚¨çš„ alice_gen.py) ===
BIT_NUM = 1
LATENT_SHAPE = (1, 4, 64, 64)
LATENT_CAPACITY = 16384 # 4*64*64 * 1
# RS è¨­å®š
N_ECC_SYMBOLS = 136 
N_DATA_BYTES_PER_BLOCK = 119
NUM_BLOCKS = 2
PAYLOAD_SIZE_BYTES = NUM_BLOCKS * N_DATA_BYTES_PER_BLOCK # 238
# Repetition è¨­å®š
REPETITION_FACTOR = 3

def load_sd_model(config_path, ckpt_path, device):
    """è¼‰å…¥ Stable Diffusion"""
    print(f"[System] è¼‰å…¥ SD æ¨¡å‹: {ckpt_path}")
    config = OmegaConf.load(config_path)
    pl_sd = torch.load(ckpt_path, map_location="cpu", weights_only=False)
    sd = pl_sd["state_dict"]
    model = instantiate_from_config(config.model)
    model.load_state_dict(sd, strict=False)
    model.to(device)
    model.eval()
    return model

def load_text_system(model_path):
    """è¼‰å…¥ GPT-2 æ–‡å­—éš±å¯«æ¨¡çµ„"""
    print(f"[System] è¼‰å…¥ Text Stego System: {model_path}")
    if not os.path.exists(model_path):
        print(f"âŒ æ‰¾ä¸åˆ° GPT-2 æ¨¡å‹: {model_path}")
        sys.exit(1)
    return TextStegoSystem(model_name=model_path)

def load_coco_prompts(json_path):
    """è®€å– MS-COCO Prompts"""
    if not os.path.exists(json_path):
        print(f"âš ï¸ æ‰¾ä¸åˆ° COCO JSON: {json_path}")
        return ["A futuristic city with flying cars"] * 100
    with open(json_path, 'r') as f:
        data = json.load(f)
    return [item['caption'] for item in data['annotations']]

def get_hybrid_ecc_payload(secret_key):
    """
    é‡ç¾ alice_gen.py çš„ Hybrid ECC ç·¨ç¢¼é‚è¼¯
    Payload -> RS(255,119) -> Repetition(3)
    """
    rsc = RSCodec(N_ECC_SYMBOLS)
    rng = np.random.RandomState(secret_key)
    
    # 1. ç”Ÿæˆéš¨æ©Ÿç§˜å¯†è¨Šæ¯
    original_secret_bytes = rng.bytes(PAYLOAD_SIZE_BYTES)
    
    # 2. RS ç·¨ç¢¼
    encoded_bytes_list = []
    for i in range(NUM_BLOCKS):
        chunk = original_secret_bytes[i*N_DATA_BYTES_PER_BLOCK : (i+1)*N_DATA_BYTES_PER_BLOCK]
        encoded_chunk = rsc.encode(chunk)
        encoded_bytes_list.append(encoded_chunk)
    encoded_bytes = b"".join(encoded_bytes_list)
    
    # è½‰ç‚º bits
    rs_coded_bits = np.unpackbits(np.frombuffer(encoded_bytes, dtype=np.uint8))
    
    # 3. Repetition ç·¨ç¢¼
    hybrid_coded_bits = np.repeat(rs_coded_bits, REPETITION_FACTOR)
    
    # 4. å¡«å…… Padding
    encoded_size_bits = len(hybrid_coded_bits)
    secret_msg_payload = np.zeros(np.prod(LATENT_SHAPE), dtype=np.uint8).flatten()
    secret_msg_payload[:encoded_size_bits] = hybrid_coded_bits
    
    # éš¨æ©Ÿå¡«å……å‰©é¤˜ç©ºé–“ (ä½¿ç”¨ä¸åŒ seed é¿å…æ··æ·†)
    seed_kernel = secret_key
    rng_pad = np.random.RandomState(seed=seed_kernel+1)
    random_padding = rng_pad.randint(0, 2**BIT_NUM, LATENT_CAPACITY - encoded_size_bits)
    secret_msg_payload[encoded_size_bits:] = random_padding
    
    return secret_msg_payload.reshape(LATENT_SHAPE).astype(np.int8)

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ [Dual System] é–‹å§‹ç”Ÿæˆè¨“ç·´æ•¸æ“š (ç›®æ¨™: {NUM_SAMPLES} å°)")
    
    # 1. åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹
    sd_model = load_sd_model(CONFIG_PATH, CKPT_PATH, device)
    sd_sampler = DPMSolverSampler(sd_model)
    text_sys = load_text_system(GPT2_PATH)
    mapper = mapping_module.ours_mapping(bits=BIT_NUM)
    
    # 2. æº–å‚™ Prompts
    all_prompts = load_coco_prompts(COCO_JSON_PATH)
    
    # 3. æº–å‚™è¼¸å‡ºç›®éŒ„
    os.makedirs(os.path.join(OUTPUT_ROOT, "cover"), exist_ok=True)
    os.makedirs(os.path.join(OUTPUT_ROOT, "stego"), exist_ok=True)
    
    # 4. ç”Ÿæˆè¿´åœˆ
    for i in tqdm(range(NUM_SAMPLES), desc="Generating"):
        filename = f"{i:05d}.png"
        cover_path = os.path.join(OUTPUT_ROOT, "cover", filename)
        stego_path = os.path.join(OUTPUT_ROOT, "stego", filename)
        
        # ä¸­æ–·çºŒå‚³
        if os.path.exists(cover_path) and os.path.exists(stego_path):
            continue
            
        # é¸å–åŸå§‹ Prompt
        origin_prompt = np.random.choice(all_prompts)
        
        # === A. ç”Ÿæˆ Cover (ç´”æ·¨ç‰ˆ) ===
        # ä½¿ç”¨åŸå§‹ COCO Prompt + ç´”éš¨æ©Ÿå™ªè²
        seed_cover = np.random.randint(0, 1000000)
        np.random.seed(seed_cover)
        noise_cover = torch.randn(1, 4, 64, 64).to(device)
        
        c_cover = sd_model.get_learned_conditioning([origin_prompt])
        uc = sd_model.get_learned_conditioning([""])
        
        with torch.no_grad(), autocast("cuda"):
            z_0_c, _ = sd_sampler.sample(
                steps=50, conditioning=c_cover, batch_size=1, shape=(4,64,64),
                unconditional_guidance_scale=5.0, unconditional_conditioning=uc,
                x_T=noise_cover
            )
            x_cover = sd_model.decode_first_stage(z_0_c)
            x_cover = torch.clamp((x_cover + 1.0) / 2.0, min=0.0, max=1.0)
            
        Image.fromarray((x_cover[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)).save(cover_path)

        # === B. ç”Ÿæˆ Stego (é›™æ¨¡æ…‹ç‰ˆ) ===
        # 1. æº–å‚™ Session Key
        session_key = int(np.random.randint(10000000, 99999999))
        
        # 2. [Text Stego] ä½¿ç”¨ GPT-2 ä¿®æ”¹ Prompt ä¸¦åµŒå…¥ Key
        try:
            # é€™æ˜¯æ‚¨çš„ dual_system_main.py é‚è¼¯
            stego_prompt_text, _ = text_sys.alice_encode(origin_prompt, session_key)
        except Exception as e:
            print(f"\nâš ï¸ Text Encode Failed: {e}, using original prompt")
            stego_prompt_text = origin_prompt
            
        # 3. [Image Stego] æº–å‚™ Payload (Hybrid ECC)
        # é€™æ˜¯æ‚¨çš„ alice_gen.py é‚è¼¯
        secret_msg = get_hybrid_ecc_payload(session_key)
        
        # 4. [Image Stego] Mapping
        seed_kernel = session_key
        seed_shuffle = (session_key + 9527) % (2**32)
        
        z_stego_np = mapper.encode_secret(
            secret_message=secret_msg,
            seed_kernel=seed_kernel,
            seed_shuffle=seed_shuffle
        )
        z_T_stego = torch.from_numpy(z_stego_np.astype(np.float32)).to(device)
        
        # 5. ç”Ÿæˆ Stego åœ–åƒ (ä½¿ç”¨ Modified Prompt + Mapped Noise)
        c_stego = sd_model.get_learned_conditioning([stego_prompt_text])
        
        with torch.no_grad(), autocast("cuda"):
            z_0_s, _ = sd_sampler.sample(
                steps=50, # æ‚¨è¨­å®š 50 steps
                conditioning=c_stego, batch_size=1, shape=(4,64,64),
                unconditional_guidance_scale=5.0, unconditional_conditioning=uc,
                x_T=z_T_stego
            )
            x_stego = sd_model.decode_first_stage(z_0_s)
            x_stego = torch.clamp((x_stego + 1.0) / 2.0, min=0.0, max=1.0)
            
        Image.fromarray((x_stego[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)).save(stego_path)

        # å®šæœŸæ¸…ç†
        if i % 50 == 0:
            torch.cuda.empty_cache()

    print("âœ… é›™æ¨¡æ…‹æ•¸æ“šç”Ÿæˆå®Œæˆï¼")

if __name__ == "__main__":
    main()